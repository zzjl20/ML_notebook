## 这是我写的第二个CNN，虽然目前尚未完成，但是比第一个，有一些不一样: 
1.使用用更多的数据。第一个cnn使用了100个数据训练。这次要使用MINST数据集，数字手写数据集<br>
2.首次使用tensorflow。tensorflow正在学，但是我没学完就想用。神经网络反向推导时候，要对每个参数求偏导数，当网络层数增加、数据特征长度增大以及添加复杂的非线性函数之后，模型的表达式将变得 非常复杂，很难手动推导出模型和梯度的计算公式。而且一旦网络结构发生变动，网络的 模型函数也随之发生改变，依赖手动计算梯度的方式显然不可行。深度学习框架有自动求导技术，设定好参数，就可以自动求导。<br>
3.首次使用谷歌的在线GPU运算(Google Colab)，估计GPU运算速度能提高不少。<br>
4.使用ReLU函数，上次使用的是sigmoid.<br>
本次的模型神经网络，有三层。输入节点784个，第一层256，第二次128， 第三城10.结构如下：<br>
    out = 𝑅𝑒𝐿𝑈{𝑅𝑒𝐿𝑈{𝑅𝑒𝐿𝑈[𝑿@𝑾1 + 𝒃1]@𝑾2 + 𝒃2}@𝑾 + 𝒃 }  <br>

