## è¿™æ˜¯æˆ‘å†™çš„ç¬¬äºŒä¸ªCNNï¼Œè™½ç„¶ç›®å‰å°šæœªå®Œæˆï¼Œä½†æ˜¯æ¯”ç¬¬ä¸€ä¸ªï¼Œæœ‰ä¸€äº›ä¸ä¸€æ ·: 
1.ä½¿ç”¨ç”¨æ›´å¤šçš„æ•°æ®ã€‚ç¬¬ä¸€ä¸ªcnnä½¿ç”¨äº†100ä¸ªæ•°æ®è®­ç»ƒã€‚è¿™æ¬¡è¦ä½¿ç”¨MINSTæ•°æ®é›†ï¼Œæ•°å­—æ‰‹å†™æ•°æ®é›†<br>
2.é¦–æ¬¡ä½¿ç”¨tensorflowã€‚tensorflowæ­£åœ¨å­¦ï¼Œä½†æ˜¯æˆ‘æ²¡å­¦å®Œå°±æƒ³ç”¨ã€‚ç¥ç»ç½‘ç»œåå‘æ¨å¯¼æ—¶å€™ï¼Œè¦å¯¹æ¯ä¸ªå‚æ•°æ±‚åå¯¼æ•°ï¼Œå½“ç½‘ç»œå±‚æ•°å¢åŠ ã€æ•°æ®ç‰¹å¾é•¿åº¦å¢å¤§ä»¥åŠæ·»åŠ å¤æ‚çš„éçº¿æ€§å‡½æ•°ä¹‹åï¼Œæ¨¡å‹çš„è¡¨è¾¾å¼å°†å˜å¾— éå¸¸å¤æ‚ï¼Œå¾ˆéš¾æ‰‹åŠ¨æ¨å¯¼å‡ºæ¨¡å‹å’Œæ¢¯åº¦çš„è®¡ç®—å…¬å¼ã€‚è€Œä¸”ä¸€æ—¦ç½‘ç»œç»“æ„å‘ç”Ÿå˜åŠ¨ï¼Œç½‘ç»œçš„ æ¨¡å‹å‡½æ•°ä¹Ÿéšä¹‹å‘ç”Ÿæ”¹å˜ï¼Œä¾èµ–æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦çš„æ–¹å¼æ˜¾ç„¶ä¸å¯è¡Œã€‚æ·±åº¦å­¦ä¹ æ¡†æ¶æœ‰è‡ªåŠ¨æ±‚å¯¼æŠ€æœ¯ï¼Œè®¾å®šå¥½å‚æ•°ï¼Œå°±å¯ä»¥è‡ªåŠ¨æ±‚å¯¼ã€‚<br>
3.é¦–æ¬¡ä½¿ç”¨è°·æ­Œçš„åœ¨çº¿GPUè¿ç®—(Google Colab)ï¼Œä¼°è®¡GPUè¿ç®—é€Ÿåº¦èƒ½æé«˜ä¸å°‘ã€‚<br>
4.ä½¿ç”¨ReLUå‡½æ•°ï¼Œä¸Šæ¬¡ä½¿ç”¨çš„æ˜¯sigmoid.<br>
æœ¬æ¬¡çš„æ¨¡å‹ç¥ç»ç½‘ç»œï¼Œæœ‰ä¸‰å±‚ã€‚è¾“å…¥èŠ‚ç‚¹784ä¸ªï¼Œç¬¬ä¸€å±‚256ï¼Œç¬¬äºŒæ¬¡128ï¼Œ ç¬¬ä¸‰åŸ10.ç»“æ„å¦‚ä¸‹ï¼š<br>
    out = ğ‘…ğ‘’ğ¿ğ‘ˆ{ğ‘…ğ‘’ğ¿ğ‘ˆ{ğ‘…ğ‘’ğ¿ğ‘ˆ[ğ‘¿@ğ‘¾1 + ğ’ƒ1]@ğ‘¾2 + ğ’ƒ2}@ğ‘¾ + ğ’ƒ }  <br>
```
# è½½å…¥åŒ…ï¼Œå¯¼å…¥æ•°æ®
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers, datasets
(x, y), (x_val, y_val) = datasets.mnist.load_data()  
# ï¼ˆx,yï¼‰æ˜¯è®­ç»ƒæ•°æ®ï¼Œxæ˜¯ç‰¹å¾ï¼Œyæ˜¯æ ‡ç­¾ã€‚åŒæ · (x_val, y_val)æ˜¯æ£€æµ‹æ•°æ®ã€‚
```
æ„é€ ä¸­é—´å±‚
```
# æ¯å±‚çš„å¼ é‡éƒ½éœ€è¦è¢«ä¼˜åŒ–ï¼Œæ•…ä½¿ç”¨Variableç±»å‹ï¼Œå¹¶ä½¿ç”¨æˆªæ–­çš„æ­£å¤ªåˆ†å¸ƒåˆå§‹åŒ–æƒå€¼å¼ é‡
# åˆå§‹çš„béƒ½èµ‹å€¼æˆ0
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1)) 
b1 = tf.Variable(tf.zeros([256]))
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1)) 
b2 = tf.Variable(tf.zeros([128]))
w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1)) 
b3 = tf.Variable(tf.zeros([10]))
```
ç¬¬ä¸€å±‚çš„w1æ˜¯784è¡Œï¼Œ256åˆ—ï¼ŒæŸ¥çœ‹ç›®å‰çš„x.shapeå¯çŸ¥ï¼Œshape=[60000,28,28]<br> 
**æ³¨æ„è¿™é‡Œæœ‰å‡ ä¸ªå‘ï¼š** 1.xæ˜¯ä¸èƒ½å’Œw1çŸ©é˜µç›¸ä¹˜çš„ã€‚ç»´åº¦å¯¹ä¸ä¸Šã€‚ä½¿ç”¨`tf.reshape(x, [-1, 28*28])`ï¼Œå°†å…¶è½¬åŒ–ä¸º60000 * 784, ä¹Ÿå°±æ˜¯æŠŠè¡Œåˆ—ç›¸ä¹˜ï¼Œçœ‹ä½œ60000ä¸ªæ··åˆç‰¹å¾çš„æ ·æœ¬ã€‚è¿™æ ·ç»´åº¦å°±å¯¹ä¸Šäº†ã€‚ 2.è½¬æ¢ä¹‹åä»ç„¶ä¸å¯ç›¸ä¹˜ã€‚ä¼šå‡ºç°:NotFoundError: Could not find valid device for node. åŸå› æ˜¯x.dtype = tf.uint8ã€‚ä½¿ç”¨`tf.cast(x, tf.float32)`å°†å…¶è½¬æ¢ã€‚ 3. è½¬æ¢ä¹‹åä»ç„¶ä¸èƒ½ä½¿ç”¨ã€‚å› ä¸ºxå–å€¼èŒƒå›´æ˜¯0~255,w1çŸ©é˜µéƒ½æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒå–çš„å€¼ï¼Œä¸¤è€…ç›¸å·®å¤ªå¤§ã€‚ è¦å°†xå–å€¼å˜æ¢åˆ°[-1ï¼Œ1]ä¹‹é—´ã€‚<br>
```
x1 = tf.reshape(x, [-1, 28*28]) # xåŸå…ˆæ˜¯numpy.ndarrayç±»å‹ï¼Œä½†æ˜¯ä¹Ÿæœ‰shapeï¼Œx.shape = [60000,28,28] 60000ä¸ª28*28çš„å›¾ç‰‡
x1 = tf.cast(x1, tf.float32)  # ç„¶åå°†xç›´æ¥å˜æˆtensor,ç„¶åè¿˜reshapeäº†ä¸€ä¸‹ï¼Œä¹‹åxå˜ä¸ºshape = [60000, 784],å¯ä»¥å’Œw1çŸ©é˜µç›¸ä¹˜ã€‚
x1 = 2*x1/255-1      #ã€€å°†intè½¬åŒ–ä¸ºfloat32ï¼Œ ç„¶åç¼©æ”¾è‡³-1ï½1

x2 = 2*tf.convert_to_tensor(x, dtype=tf.float32)/255.-1  # æ–¹æ³•ä¹‹äºŒï¼Œå…ˆç¼©æ”¾ï¼Œå†reshape
x2 = tf.reshape(x2, [-1, 28*28])

all(x1[0] == x2[0])  # å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ç§æ–¹æ³•å¾—åˆ°äº†ä¸€æ ·çš„æ•°æ®

y = tf.convert_to_tensor(y, dtype=tf.int32) # è½¬æ¢ä¸ºæ•´å½¢å¼ é‡
y = tf.one_hot(y, depth=10) # one-hotç¼–ç 

```
é‚£ä¹ˆï¼Œå¼€å§‹å‘å‰æ¨å¯¼ï¼Œç”±x->w1,b1-> w2,b2-> w3,b3 -> out.outæ˜¯ä¸ª10ç»´å‘é‡ï¼Œä¸yç›¸æ¯”å¯ä»¥å¾—åˆ°è¯¯å·®ã€‚
```
# ç¬¬ä¸€å±‚è®¡ç®—ï¼Œ[b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b,256] + [b, 256]
h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) 
h1 = tf.nn.relu(h1) # é€šè¿‡æ¿€æ´»å‡½æ•°
# ç¬¬äºŒå±‚è®¡ç®—ï¼Œ[b, 256] => [b, 128] 
h2 = h1@w2 + b2
h2 = tf.nn.relu(h2)
# è¾“å‡ºå±‚è®¡ç®—ï¼Œ[b, 128] => [b, 10] 
out = h2@w3 + b3
```
è®¡ç®—è¯¯å·®ï¼Œä½¿ç”¨MSEè¯¯å·®ã€‚mse = mean(sum(y-out)^2)
```
loss = tf.square(y_onehot - out)
loss = tf.reduce_mean(loss)
```
è®¡ç®—è‡³æ­¤ï¼Œéœ€è¦å‘å‰å›æ¨æ¢¯åº¦ï¼Œéœ€è¦è®¡ç®—æ¢¯åº¦çš„å˜é‡æœ‰[w1, b1, w2, b2, w3, b3]ã€‚è‡ªåŠ¨è®¡ç®—æ¢¯åº¦éœ€è¦`tf.GradientTape()`,å°†ä¸Šè¿°çš„è®¡ç®—è¿‡ç¨‹å…¨éƒ¨åŒ…è£¹åœ¨ä¸€ä¸ª`with tf.GradientTape() as tape`ä¸­ï¼Œå¾—åˆ°ï¼š
```
with tf.GradientTape() as tape:
    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) 
    h1 = tf.nn.relu(h1)
    h2 = h1@w2 + b2
    h2 = tf.nn.relu(h2)
    out = h2@w3 + b3         # å‘å‰æ¨å¯¼ç»“æŸ
    loss = tf.square(y_onehot - out)
    loss = tf.reduce_mean(loss)  # è®¡ç®—è¯¯å·®
    grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])  #è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
    w1.assign_sub(lr * grads[0])  # assign_sub()æ˜¯åŸåœ°æ›´æ–°çš„æ„æ€ï¼Œå°†æ‹¬å·å†…çš„æ•°å€¼èµ‹å€¼ç»™ç­‰å·åŸæ¥
    b1.assign_sub(lr * grads[1])
    w2.assign_sub(lr * grads[2]) 
    b2.assign_sub(lr * grads[3]) 
    w3.assign_sub(lr * grads[4]) 
    b3.assign_sub(lr * grads[5])  # æ›´æ–°æ‰€æœ‰å˜é‡
```
åˆ°æ­¤ä½ç½®ï¼Œæ²¡å‡ºä»€ä¹ˆé”™è¯¯ï¼Œä½†æ˜¯å½“æˆ‘åŠ ä¸Šä¸€ä¸ªloss_list = [],å¹¶ä¸”æŠŠæ¯æ¬¡losséƒ½åŠ å…¥ä¹‹åï¼Œå‘ç°loss_listé‡Œé¢åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œè®©æˆ‘å¾ˆå¥‡æ€ªï¼Œä¸çŸ¥é“æ˜¯ä»€ä¹ˆåŸå› ï¼Œå¯èƒ½æˆ‘è¿˜ä¸äº†è§£`with tf.GradientTape() as tape:`å®ƒçš„å«ä¹‰å§ã€‚ä»¥å‰æˆåŠŸè¿‡ä¸€æ¬¡ï¼Œloss_listä¸­æœ‰ä¸€ä¸ªTFå˜é‡ï¼Œå®ƒçš„shape=[60000,10],åº”è¯¥æ˜¯60000ä¸ªyä¸outçš„MSEã€‚å¯æ˜¯åªå‡ºç°è¿‡ä¸€æ¬¡æˆ‘å¤åˆ¶ä¸å‡ºæ¥äº†ã€‚<br>
ç›®å‰è¿˜æœ‰2ä¸ªä»»åŠ¡: 1. å°†è¯¯å·®éšç€è®­ç»ƒé€æ¸é™ä½è¡¨ç¤ºå‡ºæ¥ã€‚ 2.æµ‹éªŒæ•°æ®å¯¼å…¥åˆ°ç½‘ç»œä¸­ï¼Œçœ‹è¯¯å·®ã€‚<br>
# è¾“å‡ºå±‚è®¡ç®—ï¼Œ[b, 128] => [b, 10] ï¼š
# è¾“å‡ºå±‚è®¡ç®—ï¼Œ[b, 128] => [b, 10] è¯¯å·®éšç€
# è¾“å‡ºå±‚è®¡ç®—ï¼Œ[b, 128] => [b, 10] è®­ç»ƒ
# è¾“å‡ºå±‚è®¡ç®—ï¼Œ[b, 128] => [b, 10] 
