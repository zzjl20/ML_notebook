## è¿™æ˜¯æˆ‘å†™çš„ç¬¬äºŒä¸ªCNNï¼Œè™½ç„¶ç›®å‰å°šæœªå®Œæˆï¼Œä½†æ˜¯æ¯”ç¬¬ä¸€ä¸ªï¼Œæœ‰ä¸€äº›ä¸ä¸€æ ·: 
1.ä½¿ç”¨ç”¨æ›´å¤šçš„æ•°æ®ã€‚ç¬¬ä¸€ä¸ªcnnä½¿ç”¨äº†100ä¸ªæ•°æ®è®­ç»ƒã€‚è¿™æ¬¡è¦ä½¿ç”¨MINSTæ•°æ®é›†ï¼Œæ•°å­—æ‰‹å†™æ•°æ®é›†<br>
2.é¦–æ¬¡ä½¿ç”¨tensorflowã€‚tensorflowæ­£åœ¨å­¦ï¼Œä½†æ˜¯æˆ‘æ²¡å­¦å®Œå°±æƒ³ç”¨ã€‚ç¥ç»ç½‘ç»œåå‘æ¨å¯¼æ—¶å€™ï¼Œè¦å¯¹æ¯ä¸ªå‚æ•°æ±‚åå¯¼æ•°ï¼Œå½“ç½‘ç»œå±‚æ•°å¢åŠ ã€æ•°æ®ç‰¹å¾é•¿åº¦å¢å¤§ä»¥åŠæ·»åŠ å¤æ‚çš„éçº¿æ€§å‡½æ•°ä¹‹åï¼Œæ¨¡å‹çš„è¡¨è¾¾å¼å°†å˜å¾— éå¸¸å¤æ‚ï¼Œå¾ˆéš¾æ‰‹åŠ¨æ¨å¯¼å‡ºæ¨¡å‹å’Œæ¢¯åº¦çš„è®¡ç®—å…¬å¼ã€‚è€Œä¸”ä¸€æ—¦ç½‘ç»œç»“æ„å‘ç”Ÿå˜åŠ¨ï¼Œç½‘ç»œçš„ æ¨¡å‹å‡½æ•°ä¹Ÿéšä¹‹å‘ç”Ÿæ”¹å˜ï¼Œä¾èµ–æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦çš„æ–¹å¼æ˜¾ç„¶ä¸å¯è¡Œã€‚æ·±åº¦å­¦ä¹ æ¡†æ¶æœ‰è‡ªåŠ¨æ±‚å¯¼æŠ€æœ¯ï¼Œè®¾å®šå¥½å‚æ•°ï¼Œå°±å¯ä»¥è‡ªåŠ¨æ±‚å¯¼ã€‚<br>
3.é¦–æ¬¡ä½¿ç”¨è°·æ­Œçš„åœ¨çº¿GPUè¿ç®—(Google Colab)ï¼Œä¼°è®¡GPUè¿ç®—é€Ÿåº¦èƒ½æé«˜ä¸å°‘ã€‚<br>
4.ä½¿ç”¨ReLUå‡½æ•°ï¼Œä¸Šæ¬¡ä½¿ç”¨çš„æ˜¯sigmoid.<br>
æœ¬æ¬¡çš„æ¨¡å‹ç¥ç»ç½‘ç»œï¼Œæœ‰ä¸‰å±‚ã€‚è¾“å…¥èŠ‚ç‚¹784ä¸ªï¼Œç¬¬ä¸€å±‚256ï¼Œç¬¬äºŒæ¬¡128ï¼Œ ç¬¬ä¸‰åŸ10.ç»“æ„å¦‚ä¸‹ï¼š<br>
    out = ğ‘…ğ‘’ğ¿ğ‘ˆ{ğ‘…ğ‘’ğ¿ğ‘ˆ{ğ‘…ğ‘’ğ¿ğ‘ˆ[ğ‘¿@ğ‘¾1 + ğ’ƒ1]@ğ‘¾2 + ğ’ƒ2}@ğ‘¾ + ğ’ƒ }  <br>
```
# è½½å…¥åŒ…ï¼Œå¯¼å…¥æ•°æ®
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers, datasets
(x, y), (x_val, y_val) = datasets.mnist.load_data()  
# ï¼ˆx,yï¼‰æ˜¯è®­ç»ƒæ•°æ®ï¼Œxæ˜¯ç‰¹å¾ï¼Œyæ˜¯æ ‡ç­¾ã€‚åŒæ · (x_val, y_val)æ˜¯æ£€æµ‹æ•°æ®ã€‚
```
æ„é€ ä¸­é—´å±‚
```
# æ¯å±‚çš„å¼ é‡éƒ½éœ€è¦è¢«ä¼˜åŒ–ï¼Œæ•…ä½¿ç”¨Variableç±»å‹ï¼Œå¹¶ä½¿ç”¨æˆªæ–­çš„æ­£å¤ªåˆ†å¸ƒåˆå§‹åŒ–æƒå€¼å¼ é‡
# åˆå§‹çš„béƒ½èµ‹å€¼æˆ0
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1)) 
b1 = tf.Variable(tf.zeros([256]))
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1)) 
b2 = tf.Variable(tf.zeros([128]))
w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1)) 
b3 = tf.Variable(tf.zeros([10]))
```
ç¬¬ä¸€å±‚çš„w1æ˜¯784è¡Œï¼Œ256åˆ—ï¼ŒæŸ¥çœ‹ç›®å‰çš„x.shapeå¯çŸ¥ï¼Œshape=[60000,28,28]<br> 
**æ³¨æ„è¿™é‡Œæœ‰å‡ ä¸ªå‘ï¼š** 1.xæ˜¯ä¸èƒ½å’Œw1çŸ©é˜µç›¸ä¹˜çš„ã€‚ç»´åº¦å¯¹ä¸ä¸Šã€‚ä½¿ç”¨`tf.reshape(x, [-1, 28*28])`ï¼Œå°†å…¶è½¬åŒ–ä¸º60000 * 784, ä¹Ÿå°±æ˜¯æŠŠè¡Œåˆ—ç›¸ä¹˜ï¼Œçœ‹ä½œ60000ä¸ªæ··åˆç‰¹å¾çš„æ ·æœ¬ã€‚è¿™æ ·ç»´åº¦å°±å¯¹ä¸Šäº†ã€‚ 2.è½¬æ¢ä¹‹åä»ç„¶ä¸å¯ç›¸ä¹˜ã€‚ä¼šå‡ºç°:NotFoundError: Could not find valid device for node. åŸå› æ˜¯x.dtype = tf.uint8ã€‚ä½¿ç”¨`tf.cast(x, tf.float32)`å°†å…¶è½¬æ¢ã€‚ 3. è½¬æ¢ä¹‹åä»ç„¶ä¸èƒ½ä½¿ç”¨ã€‚å› ä¸ºxå–å€¼èŒƒå›´æ˜¯0~255,w1çŸ©é˜µéƒ½æ˜¯æ ‡å‡†æ­£å¤ªåˆ†å¸ƒå–çš„å€¼ï¼Œä¸¤è€…ç›¸å·®å¤ªå¤§ã€‚ è¦å°†xå–å€¼å˜æ¢åˆ°[-1ï¼Œ1]ä¹‹é—´ã€‚<br>
```
x1 = tf.reshape(x, [-1, 28*28]) # xåŸå…ˆæ˜¯numpy.ndarrayç±»å‹ï¼Œä½†æ˜¯ä¹Ÿæœ‰shapeï¼Œx.shape = [60000,28,28] 60000ä¸ª28*28çš„å›¾ç‰‡
x1 = tf.cast(x1, tf.float32)  # ç„¶åå°†xç›´æ¥å˜æˆtensor,ç„¶åè¿˜reshapeäº†ä¸€ä¸‹ï¼Œä¹‹åxå˜ä¸ºshape = [60000, 784],å¯ä»¥å’Œw1çŸ©é˜µç›¸ä¹˜ã€‚
x1 = 2*x1/255-1      #ã€€å°†intè½¬åŒ–ä¸ºfloat32ï¼Œ ç„¶åç¼©æ”¾è‡³-1ï½1
x2 = 2*tf.convert_to_tensor(x, dtype=tf.float32)/255.-1  # æ–¹æ³•ä¹‹äºŒï¼Œå…ˆç¼©æ”¾ï¼Œå†reshape
x2 = tf.reshape(x2, [-1, 28*28])

all(x1[0] == x2[0])  # å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ç§æ–¹æ³•å¾—åˆ°äº†ä¸€æ ·çš„æ•°æ®

y = tf.convert_to_tensor(y, dtype=tf.int32) # è½¬æ¢ä¸ºæ•´å½¢å¼ é‡
y = tf.one_hot(y, depth=10) # one-hotç¼–ç 

```
é‚£ä¹ˆï¼Œå¼€å§‹å‘å‰æ¨å¯¼ï¼Œç”±x->w1,b1-> w2,b2-> w3,b3 -> out.outæ˜¯ä¸ª10ç»´å‘é‡ï¼Œä¸yç›¸æ¯”å¯ä»¥å¾—åˆ°è¯¯å·®ã€‚
```
# ç¬¬ä¸€å±‚è®¡ç®—ï¼Œ[60000, 784]@[784, 256] + [256] => [60000, 256] + [256] => [60000,256] + [60000, 256]
h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) 
h1 = tf.nn.relu(h1) # é€šè¿‡æ¿€æ´»å‡½æ•°
# ç¬¬äºŒå±‚è®¡ç®—ï¼Œ[60000, 256] => [60000, 128] 
h2 = h1@w2 + b2
h2 = tf.nn.relu(h2)
# è¾“å‡ºå±‚è®¡ç®—ï¼Œ[60000, 128] => [60000, 10] 
out = h2@w3 + b3
```
è®¡ç®—è¯¯å·®ï¼Œä½¿ç”¨MSEè¯¯å·®ã€‚mse = mean(sum(y-out)^2)
```
loss = tf.square(y_onehot - out)
loss = tf.reduce_mean(loss)
```
è®¡ç®—è‡³æ­¤ï¼Œéœ€è¦å‘å‰å›æ¨æ¢¯åº¦ï¼Œéœ€è¦è®¡ç®—æ¢¯åº¦çš„å˜é‡æœ‰[w1, b1, w2, b2, w3, b3]ã€‚è‡ªåŠ¨è®¡ç®—æ¢¯åº¦éœ€è¦`tf.GradientTape()`,å°†ä¸Šè¿°çš„è®¡ç®—è¿‡ç¨‹å…¨éƒ¨åŒ…è£¹åœ¨ä¸€ä¸ª`with tf.GradientTape() as tape`ä¸­ï¼Œå¾—åˆ°ï¼š
```
with tf.GradientTape() as tape:
    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) 
    h1 = tf.nn.relu(h1)
    h2 = h1@w2 + b2
    h2 = tf.nn.relu(h2)
    out = h2@w3 + b3         # å‘å‰æ¨å¯¼ç»“æŸ
    loss = tf.square(y_onehot - out)
    loss = tf.reduce_mean(loss)  # è®¡ç®—è¯¯å·®
    grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])  #è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
    w1.assign_sub(lr * grads[0])  # assign_sub()æ˜¯åŸåœ°æ›´æ–°çš„æ„æ€ï¼Œå°†æ‹¬å·å†…çš„æ•°å€¼èµ‹å€¼ç»™ç­‰å·åŸæ¥
    b1.assign_sub(lr * grads[1])  # lr å°±æ˜¯æ­¥é•¿ã€‚ è¦åœ¨withä¹‹å¤–è®¾ç½®lr
    w2.assign_sub(lr * grads[2]) 
    b2.assign_sub(lr * grads[3]) 
    w3.assign_sub(lr * grads[4]) 
    b3.assign_sub(lr * grads[5])  # æ›´æ–°æ‰€æœ‰å˜é‡
```
åˆ°æ­¤ä½ç½®ï¼Œæ²¡å‡ºä»€ä¹ˆé”™è¯¯ï¼Œä½†æ˜¯å½“æˆ‘åŠ ä¸Šä¸€ä¸ªloss_list = [],å¹¶ä¸”æŠŠæ¯æ¬¡losséƒ½åŠ å…¥ä¹‹åï¼Œå‘ç°loss_listé‡Œé¢åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œè®©æˆ‘å¾ˆå¥‡æ€ªï¼Œä¸çŸ¥é“æ˜¯ä»€ä¹ˆåŸå› ï¼Œå¯èƒ½æˆ‘è¿˜ä¸äº†è§£`with tf.GradientTape() as tape:`å®ƒçš„å«ä¹‰å§ã€‚~~ä»¥å‰æˆåŠŸè¿‡ä¸€æ¬¡ï¼Œloss_listä¸­æœ‰ä¸€ä¸ªTFå˜é‡ï¼Œå®ƒçš„shape=[60000,10],åº”è¯¥æ˜¯60000ä¸ªyä¸outçš„MSEã€‚å¯æ˜¯åªå‡ºç°è¿‡ä¸€æ¬¡æˆ‘å¤åˆ¶ä¸å‡ºæ¥äº†ã€‚~~ è¿™æ˜¯ä¸ªé”™è¯¯çš„æ“ä½œï¼Œåªèƒ½å¾—åˆ°ä¸€ä¸ªå€¼ã€‚æˆ‘ä»¥ä¸ºè¿™å°±å¯ä»¥è®­ç»ƒå¥½äº†ï¼Œä½†å…¶å®ä¸æ˜¯ï¼Œè¿™å°±æ˜¯ä¸€æ¬¡å‘å‰+åå‘+Wï¼ŒBè°ƒæ•´ã€‚<br>
ç›®å‰è¿˜æœ‰2ä¸ªä»»åŠ¡: 1. å°†è¯¯å·®éšç€è®­ç»ƒé€æ¸é™ä½è¡¨ç¤ºå‡ºæ¥ã€‚ 2.æµ‹éªŒæ•°æ®å¯¼å…¥åˆ°ç½‘ç»œä¸­ï¼Œçœ‹è¯¯å·®ã€‚<br>
æˆ‘å¥½åƒæ˜ç™½å“ªé‡Œä¸å¯¹äº†ã€‚ä¸Šè¿°ä¸€ä¸ª`with tf.GradientTape() as tape:`ï¼Œåªè®¡ç®—é‡ä¸€æ¬¡ï¼Œå‘å‰æ¨å¯¼-> å¾—åˆ°è¯¯å·®-> æ›´æ–°Wï¼ŒBï¼Œæ‰€ä»¥åªæ˜¯å¾—åˆ°äº†ä¸€ä¸ªè¯¯å·®ã€‚æ‰€ä»¥ä¹Ÿæ²¡lossæ›²çº¿ä»€ä¹ˆã€‚å¦å¤–`tf.reduce_mean(loss)`æ˜¯æ±‚å¹³å‡æ•°çš„æ„æ€ã€‚å¦‚æœæƒ³è¦çœ‹åˆ°è¯¯å·®è¶‹åŠ¿ï¼ŒæŠŠä¸Šé¢çš„`with`å¤šåšå‡ æ¬¡ï¼Œå°±æ˜¯å¾ªç¯ï¼Œå°±å¯ä»¥çœ‹åˆ°lossçš„å˜åŒ–äº†ã€‚ å¦å¤–è¿˜æœ‰ä¸€ç‚¹ï¼Œlrè¦è®¾ç½®çš„å°ä¸€ç‚¹ï¼Œæˆ‘åˆå§‹è®¾ç½®ä¸º0.1ï¼Œç»“æœ`with`4ï¼Œ5éï¼Œè¯¯å·®å°±å˜æˆæ— é™å¤§äº†ã€‚è®¾ç½®æˆ0.01æ˜¯åº•çº¿ï¼Œ0ï¼Œ001ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚<br>
ç»¼ä¸Šï¼Œä¸€ä¸ª`with`å¤–é¢è¿˜è¦å¥—ä¸Šå¤šæ¬¡è®­ç»ƒçš„å¾ªç¯ï¼Œæˆ–è€…æŒ‡å®šè¯¯å·®ä¸‹é™åœæ­¢é™åˆ¶ï¼Œæ‰èƒ½çœ‹åˆ°lossçš„é€æ¸å˜å°ã€‚å¦å¤–lrçš„è®¾ç½®ï¼Œä¹¦é‡Œä¹Ÿæ²¡å†™ã€‚~~ä»`with`åˆ°æœ€åçš„è¯¯å·®ä¸‹é™å›¾ï¼Œæ•´æ•´éš”äº†å¥½å‡ æ­¥ï¼Œä¹¦ä¸­éƒ½è·³è¿‡äº†ã€‚çœŸæ˜¯è¯¸è‘›è¿å‘ã€‚~~<br>
è§£å†³ä¹‹åæ‰€æœ‰é—®é¢˜ä¹‹åçš„å®Œæ•´ä»£ç :
```
# å¯¼å…¥åŒ…ï¼Œ å¯¼å…¥æ•°æ®
import tensorflow as tf
import matplotlib.pyplot as plt
(x, y), (x_val, y_val) = datasets.mnist.load_data()
x = tf.reshape(x, [-1, 28*28])   
x = tf.cast(x, tf.float32)      
x = 2*x/255-1    
y = tf.convert_to_tensor(y, dtype=tf.int32) 
y = tf.one_hot(y, depth=10) 

# åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼Œ åˆå§‹åŒ–è®­ç»ƒè½®æ•°ï¼Œæ­¥é•¿
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1)) 
b1 = tf.Variable(tf.zeros([256]))
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1)) 
b2 = tf.Variable(tf.zeros([128]))
w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1)) 
b3 = tf.Variable(tf.zeros([10]))
loss_list= []  
Epoch = 1000   # æ€»è®­ç»ƒè½®æ•°
lr = 0.003     # æ¯æ¬¡è°ƒæ•´çš„æ­¥é•¿

# è®­ç»ƒå¼€å§‹
for i in range(Epoch):
  with tf.GradientTape() as tape:
    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) 
    h1 = tf.nn.relu(h1) 
    h2 = h1@w2 + b2
    h2 = tf.nn.relu(h2ï¼‰
    out = h2@w3 + b3
    loss = tf.square(y- out)
    loss = tf.reduce_mean(loss)
    if i% 20 == 0:    
      loss_list.append(loss)    #æ¯éš”20æ¬¡å¾ªç¯è®°å½•ä¸€æ¬¡è¯¯å·®
    grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])
    w1.assign_sub(lr * grads[0]) 
    b1.assign_sub(lr * grads[1])
    w2.assign_sub(lr * grads[2]) 
    b2.assign_sub(lr * grads[3]) 
    w3.assign_sub(lr * grads[4]) 
    b3.assign_sub(lr * grads[5])

# æç»˜è¯¯å·®
plt.plot(loss_list)
plt.show()
print(loss_list[-1])

# éªŒè¯é˜¶æ®µ
x_val = 2*tf.convert_to_tensor(x_val)/255-1  
x_val = tf.reshape(x_val, [-1,28*28])
h1 = x_val@w1+b1
h1 = tf.nn.relu(h1)
h2 = h1@w2+b2
h2 = tf.nn.relu(h2)
y_pred = h2@w3+b3 

# å¾—åˆ°çš„é¢„æµ‹å€¼ï¼Œè§£ç ï¼Œå¹¶ä¸çœŸå®å€¼æ¯”è¾ƒã€‚
decode_y =  tf.argmax(y_pred, axis = 1)
print("é¢„æµ‹ç»“æœï¼š\n", decode_y[:100])
print("çœŸå®å€¼ï¼š\n", y_val[:100])  #çœ‹ä¸€ä¸‹å‰100ä¸ªçš„é¢„æµ‹ç»“æœã€‚
```
ç¥ç»ç½‘ç»œçš„è¯¯å·®æ˜¯`tf.Tensor(0.09139292, shape=(), dtype=float32)`,å¤§æ¦‚æ˜¯0.1å·¦å³çš„è¯¯å·®
<img src = "./pics/loss_list.png"><br>
æŸ¥çœ‹é¢„æµ‹çš„å‰100ä¸ªé¢„æµ‹ä¸çœŸå®çš„æ¯”å¯¹ç»“æœ: çœ‹å¾—å‡ºæ¥ï¼Œè¿˜æ˜¯æœ‰ä¸å°‘é”™è¯¯çš„ã€‚<br>
```
é¢„æµ‹ç»“æœï¼š
 tf.Tensor(
[7 1 1 6 4 1 9 1 4 9 0 1 9 0 1 0 4 7 2 0 9 6 1 8 9 9 6 4 0 1 3 1 1 4 7 1 7
 1 3 1 1 7 4 1 1 9 6 9 4 4 6 3 4 9 6 6 4 1 9 9 0 2 9 9 7 9 1 4 9 0 7 0 2 8
 1 9 9 9 9 7 9 9 2 9 4 4 7 3 6 1 3 6 9 9 1 4 1 7 1 7], shape=(100,), dtype=int64)
çœŸå®å€¼ï¼š
 [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7
 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9
 1 7 3 2 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9]
```
æµ‹è¯•ä¸€ä¸‹åˆ°åº•å‡†ç¡®ç‡æ˜¯å¤šå°‘ï¼š `æµ‹è¯•æ•°æ®å‡†ç¡®ç‡:  0.4672` è¿˜è¡Œï¼Œå¦‚æœæ˜¯éšæœºçŒœçš„è¯å‡†ç¡®ç‡åº”è¯¥æ˜¯0.1(ç»è¿‡å®éªŒï¼Œè¿™ä¸ªå‡†ç¡®ç‡æœ‰é”™ã€‚å› ä¸ºæˆ‘æ€ä¹ˆå¢åŠ å¾ªç¯æ¬¡æ•°ï¼Œå‡å°äº†è®­ç»ƒæ—¶çš„è¯¯å·®ï¼Œé¢„æµ‹ç»“æœéƒ½æ˜¯è¿™æ ·ï¼Œä¸å˜ã€‚æ‰€ä»¥å‡†ç¡®ç‡ä»ç„¶æ˜¯0.46.æˆ‘ç›®å‰è¿˜ä¸çŸ¥é“åŸå› ã€‚)<br>
```
count = (decode_y == y_val).numpy()
total = sum(count)
acc_val = total / 10000
print("æµ‹è¯•æ•°æ®å‡†ç¡®ç‡: ", acc_val)
```
### æ€»ç»“
1. æ–°æŠ€æœ¯ä½¿ç”¨äº†google driveçš„GPUè¿ç®—ï¼Œåˆç”¨äº†TFè‡ªåŠ¨æ±‚å¯¼ã€‚é€Ÿåº¦å¤§å¤§åŠ å¿«äº†
2. å¯¹æœ€åéªŒè¯é˜¶æ®µï¼Œæ˜¯è‡ªå·±æ‘¸ç´¢çš„ï¼Œä¸çŸ¥é“æ˜¯å¦æ­£ç¡®ã€‚
3. æ­¥é•¿è¿‡å¤§ä¸å…‰æ˜¯è¿ˆè¿‡æå€¼ç‚¹è¿™ä¹ˆç®€å•ï¼Œè¿˜èƒ½å¯¼è‡´è¯¯å·®è¶Šæ¥è¶Šå¤§ã€‚

